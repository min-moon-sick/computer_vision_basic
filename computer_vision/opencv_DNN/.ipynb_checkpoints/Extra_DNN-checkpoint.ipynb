{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44aee82a",
   "metadata": {},
   "source": [
    "## Googlenet 영상인식 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75dd1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opencv Deep learning Tutorial\n",
    "# https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "\n",
    "# Caffe Model Zoo : github.com/BVLC/caffe\n",
    "## 모델 파일 : dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\n",
    "## 설정 파일 : github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt\n",
    "\n",
    "\n",
    "# ONNX Model Zoo : github.com/onnx/models\n",
    "# 모델파일: https://github.com/onnx/models/tree/master/vision/classification/inception_and_googlenet/googlenet\n",
    "\n",
    "# 클래스 이름 파일 : github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/\n",
    "\n",
    "# readNet(model, config)\n",
    "# model, config\n",
    "\n",
    "# 실행순서\n",
    "# cv2.dnn.readNet(model, config)-> ret, 객체생성\n",
    "# blobFromImage(image, scalefactor, size, mean, swapRB, crop) -> retval\n",
    "# scalefactor: Multiply by factor\n",
    "# image has BGR ordering and swapRB is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f061b7dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-037f61b096e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "filename1 = 'googlenet/fig/apple2.png'\n",
    "filename2 = 'googlenet/fig/apple1.png'\n",
    "# filename = 'googlenet/fig/fish.jpg'\n",
    "\n",
    "img = cv2.imread(filename1)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "if img is None:\n",
    "    print('image load failed')\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "## 네트워크 불러오기\n",
    "# caffe \n",
    "model = './googlenet/bvlc_googlenet.caffemodel'\n",
    "config = './googlenet/deploy.prototxt.txt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net read failed')\n",
    "    sys.exit()\n",
    "\n",
    "## className\n",
    "classNames = []\n",
    "\n",
    "with open('googlenet/classification_classes_ILSVRC2012.txt', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "print(len(classNames))\n",
    "\n",
    "    \n",
    "    \n",
    "blob = cv2.dnn.blobFromImage(img, 1,(224, 224), (104, 117, 123),\n",
    "                            swapRB = False)\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "# print(prob[:10])\n",
    "\n",
    "## 결과표시하기\n",
    "\n",
    "out = prob.flatten()\n",
    "classid = np.argmax(out)\n",
    "print(classid)\n",
    "confidence = out[classid]\n",
    "\n",
    "\n",
    "text = '({}, {}%)'.format(classNames[classid], round(confidence*100, 2))\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "           0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74486cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple1.png', 'apple2.png', 'beagle.jpg', 'fish.jpg', 'pineapple.jpg', 'scooter.jpg', 'space_shuttle.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_lists = os.listdir('./googlenet/fig/')\n",
    "print(file_lists)\n",
    "\n",
    "file_names = []\n",
    "for i in file_lists:\n",
    "    file_name = './googlenet/fig/' + i\n",
    "    file_names.append(file_name)\n",
    "    \n",
    "# print(file_names)\n",
    "\n",
    "######### 네트워크 불러오기\n",
    "\n",
    "# Caffe\n",
    "model = 'googlenet/bvlc_googlenet.caffemodel'\n",
    "config = 'googlenet/deploy.prototxt'\n",
    "\n",
    "# ONNX\n",
    "# model = 'googlenet/googlenet-9.onnx'\n",
    "# config = ''\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "########## 클래스 이름 불러오기\n",
    "\n",
    "classNames = []\n",
    "with open('googlenet/classification_classes_ILSVRC2012.txt', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "turn = len(file_names)\n",
    "idx = 0\n",
    "while True:\n",
    "    img = cv2.imread(file_names[idx])\n",
    "    \n",
    "    if img is None:\n",
    "        print('image read failed')\n",
    "        break\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(img, 1, (224, 224), (104, 117, 123),\n",
    "                                swapRB = False)\n",
    "    net.setInput(blob)\n",
    "    prob = net.forward()\n",
    "    \n",
    "    out = prob.flatten()\n",
    "    _, maxval, _, maxid = cv2.minMaxLoc(out)\n",
    "\n",
    "    text = '{}, {} %'.format(classNames[maxid[1]], round(maxval*100, 2))\n",
    "        \n",
    "    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    cv2.imshow('image', img)\n",
    "    \n",
    "    if cv2.waitKey(5000) == 27:\n",
    "        break\n",
    "        \n",
    "    idx += 1\n",
    "    \n",
    "    if idx >= turn:\n",
    "        idx = 0\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "#     classid = np.argmax()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f396419",
   "metadata": {},
   "source": [
    "## OpenCV DNN 얼굴검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00bfe2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n",
    "# deploy.prototxt.txt, download-weights.py.txt, opencv_face_detector.pbtxt.text 다운로드\n",
    "\n",
    "# Caffe    https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "\n",
    "# Tensorflow  https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180220_uint8/opencv_face_detector_uint8.pb\n",
    "\n",
    "## 참고 사이트\n",
    "# https://deep-learning-study.tistory.com/299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b7b169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('opencv_face_detector/fig/sunglass.png')\n",
    "\n",
    "if img is None:\n",
    "    print('image read failed')\n",
    "    sys.exit()\n",
    "    \n",
    "model = './opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = './opencv_face_detector/deploy.prototxt'\n",
    "\n",
    "face_net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if face_net.empty():\n",
    "    print('Net open failed')\n",
    "    sys.exit()\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (300, 300), (104, 177, 123),\n",
    "                            swapRB=False)\n",
    "\n",
    "face_net.setInput(blob)\n",
    "out = face_net.forward()\n",
    "\n",
    "detect = out[0, 0, :, :]\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "for i in range(detect.shape[0]):\n",
    "    confidence = detect[i, 2]\n",
    "    \n",
    "    if confidence > 0.5:\n",
    "        x1 = int(detect[i, 3]*w)\n",
    "        y1 = int(detect[i, 4]*h)\n",
    "        x2 = int(detect[i, 5]*w)\n",
    "        y2 = int(detect[i, 6]*h)\n",
    "        \n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2),\n",
    "                     (0, 0, 255))\n",
    "        \n",
    "        text = 'Face: {}%'.format(round(confidence, 2))\n",
    "        cv2.putText(img, text, (x1, y1-1), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey() == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25b905",
   "metadata": {},
   "source": [
    "## OpenCV DNN webcam 얼굴검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "407550a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "## Caffe 학습모델\n",
    "model = 'opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = 'opencv_face_detector/deploy.prototxt'\n",
    "\n",
    "## Tensorflow 학습모델\n",
    "# model = 'opencv_face_detector/opencv_face_detector_uint8.pb'\n",
    "# config = 'opencv_face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    net.setInput(blob)\n",
    "    out = net.forward()# out.shape=(1,1, 200, 7)\n",
    "    \n",
    "        \n",
    "    detect = out[0, 0, :, :] ##0, 0, 사용안함\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "           \n",
    "            # out matrix에서 x1, y1, x2, y2 값이 0 ~1로 normalize 되어 있음\n",
    "            x1 = int(detect[i, 3] * w)\n",
    "            y1 = int(detect[i, 4] * h)\n",
    "            x2 = int(detect[i, 5] * w)\n",
    "            y2 = int(detect[i, 6] * h)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0))\n",
    "\n",
    "            label = f'Face: {confidence:4.2f}'\n",
    "            cv2.putText(frame, label, (x1, y1 - 1), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622ed20",
   "metadata": {},
   "source": [
    "## YOLOv3를 이용한 객체 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b8081d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "# NMSBoxes(bboxes, scores, score_threshold, nms_threshold) -> indices\n",
    "# nms_threshold: nms_threshold a threshold used in non maximum suppression\n",
    "\n",
    "# getPerfProfile() -> retval, timings\n",
    "# .   @brief Returns overall time for inference and timings (in ticks) for layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23115250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "model = './yolo_v3/yolov3.weights'\n",
    "config = './yolo_v3/yolov3.cfg'\n",
    "class_labels = './yolo_v3/coco.names'\n",
    "\n",
    "\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.4\n",
    "\n",
    "\n",
    "img_files = glob.glob('./yolo_v3/fig/*.*')\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed')\n",
    "    sys.exit()\n",
    "\n",
    "classes= []\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "colors = np.random.uniform(0, 255, size = (len(classes), 3))\n",
    "# print(colors)\n",
    "\n",
    "layer_names = net.getLayerNames() # result layer names\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# output_layers = ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "for i in img_files:\n",
    "    img = cv2.imread(i)\n",
    "    \n",
    "    if img is None:\n",
    "        continue\n",
    "        \n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255., (320, 320),\n",
    "                                swapRB = True)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # outs는 3개의 ndarray 리스트.\n",
    "    # outs[0].shape=(507, 85), 13*13*3=507\n",
    "    # outs[1].shape=(2028, 85), 26*26*3=2028\n",
    "    # outs[2].shape=(8112, 85), 52*52*3=8112\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > confThreshold:\n",
    "                # 바운딩 박스 중심 좌표 & 박스 크기\n",
    "                cx = int(detection[0]*w)\n",
    "                cy = int(detection[1]*h)                \n",
    "                bw = int(detection[2]*w)                \n",
    "                bh = int(detection[3]*h)\n",
    "                \n",
    "                # 바운딩 박스 좌상단 좌표\n",
    "                sx = int(cx - bw / 2)\n",
    "                sy = int(cy - bh / 2)\n",
    "                \n",
    "                boxes.append([sx, sy, bw, bh])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(int(class_id))\n",
    "                \n",
    "                 # 비최대 억제, Non Max Suppression\n",
    "#     https://www.visiongeek.io/2018/07/yolo-object-detection-opencv-python.html\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        sx, sy, bw, bh = boxes[i]\n",
    "        \n",
    "        label = '{} : {}'.format(classes[class_ids[i]], round(confidences[i], 2))\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (sx, sy, bw, bh), color, 2)\n",
    "        cv2.putText(img, label, (sx, sy-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    t,_ = net.getPerfProfile()\n",
    "    label = 'inference time: {}'.format(t * 1000.0/cv2.getTickFrequency())\n",
    "    cv2.putText(img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57841fe4",
   "metadata": {},
   "source": [
    "## YOLOv3를 이용한 동영상 객체 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01642078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "model = './yolo_v3/yolov3.weights'\n",
    "config = './yolo_v3/yolov3.cfg'\n",
    "class_labels = './yolo_v3/coco.names'\n",
    "\n",
    "\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.4\n",
    "\n",
    "\n",
    "# img_files = glob.glob('./yolo_v3/fig/*.*')\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed')\n",
    "    sys.exit()\n",
    "\n",
    "classes= []\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "colors = np.random.uniform(0, 255, size = (len(classes), 3))\n",
    "# print(colors)\n",
    "\n",
    "layer_names = net.getLayerNames() # result layer names\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# output_layers = ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "cap = cv2.VideoCapture('./yolo_v3/fig/video2.mp4')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('video open failed')\n",
    "    sys.exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame  = cap.read()\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255., (320, 320),\n",
    "                                swapRB = True)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # outs는 3개의 ndarray 리스트.\n",
    "    # outs[0].shape=(507, 85), 13*13*3=507\n",
    "    # outs[1].shape=(2028, 85), 26*26*3=2028\n",
    "    # outs[2].shape=(8112, 85), 52*52*3=8112\n",
    "    \n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > confThreshold:\n",
    "                # 바운딩 박스 중심 좌표 & 박스 크기\n",
    "                cx = int(detection[0]*w)\n",
    "                cy = int(detection[1]*h)                \n",
    "                bw = int(detection[2]*w)                \n",
    "                bh = int(detection[3]*h)\n",
    "                \n",
    "                # 바운딩 박스 좌상단 좌표\n",
    "                sx = int(cx - bw / 2)\n",
    "                sy = int(cy - bh / 2)\n",
    "                \n",
    "                boxes.append([sx, sy, bw, bh])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(int(class_id))\n",
    "                \n",
    "                 # 비최대 억제, Non Max Suppression\n",
    "#     https://www.visiongeek.io/2018/07/yolo-object-detection-opencv-python.html\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        sx, sy, bw, bh = boxes[i]\n",
    "        \n",
    "        label = '{} : {}'.format(classes[class_ids[i]], round(confidences[i], 2))\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(frame, (sx, sy, bw, bh), color, 2)\n",
    "        cv2.putText(frame, label, (sx, sy + bh + 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    t,_ = net.getPerfProfile()\n",
    "    label = 'inference time: {}'.format(t * 1000.0/cv2.getTickFrequency())\n",
    "    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('img', frame)\n",
    "    \n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d1192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
